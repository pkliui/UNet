# Setting up experiment for training

* Whilst the framework provides a UNet model pre-trained on the PH2 dataset, it is also possible to train a new UNet model using your own data, for example. For that, you will need a configuration file specifying different parameters.

* For example, to train our model on PH2 data, we used the following *ph2_config.yaml* file:
```
datapath: 'repos_data/UNet/PH2_Dataset_images/PH2/' # directory with data
output_dir: 'repos_data/UNet/runs/' #output directory to keep models
size_images:  [572, 572] # size of input images 
size_masks:  [388, 388] # size of input segmented images
validation_split: 0.25 # validation split
test_split: 0.25 # test split
batch_size: 1 # batch size
learning_rate: 1e-1 # learning rate
max_epochs: 1 # number of epochs
scheduler_step: 50 # scheduler step
scheduler_gamma: 0.1 # scheduler gamma
early_stop_patience: 2 # early stopping patience
```

## Using default experiment configuration

* ```main.py```file provides a default s

* Call your code from the command line 
``` python

python UNet/main.py /path/to/ph2_config.yaml

```

## Customizing default configuration

* Create a new dataset 
```
# create a new unet dataset and dataloader 
ph2_dataset = PH2Dataset(
    root_dir=datapath,
    transform=nn.Sequential(Resize(size_images, size_masks)))

# Create the corresponding dataloader for training and validation
data_loader = BaseDataLoader(dataset=ph2_dataset,
                                batch_size=batch_size,
                                validation_split=validation_split,
                                test_split=test_split,
                                random_seed_split=42
                                )
```

* Create data loader 

<details>
<summary>Here is how the full version of the training code in ```main.py``` could look like: </summary>

```
# import packages
import argparse
import os
import subprocess
import threading
import torch
import torch.nn as nn
import torch.optim as optim
import yaml

from UNet.data_handling.base import BaseDataLoader
from UNet.models.unet import UNet
from UNet.training.base_trainer import BaseTrainer
from UNet.data_handling.ph2dataset import PH2Dataset
from UNet.metric.metric import iou_tgs_challenge
from UNet.utils.utils import make_directory
from UNet.classes.preprocess import Resize


def get_args():
    """Get command-line arguments."""
    parser = argparse.ArgumentParser(description='Train model using config file')
    parser.add_argument('config_path', metavar='CONFIG_PATH', type=str,
                        help='Path to YAML config file')
    args = parser.parse_args()
    return args


if __name__ == '__main__':
    args = get_args()
    config_path = args.config_path
    print(f"Using config file: {config_path}")

    # Load configuration file and parameters
    with open(config_path, 'r') as f:
        config = yaml.load(f, Loader=yaml.FullLoader)

    datapath = config['datapath']
    output_dir = config['output_dir']
    size_images = tuple(config['size_images'])
    size_masks = tuple(config['size_masks'])
    validation_split = config['validation_split']
    test_split = config['test_split']
    batch_size = config['batch_size']
    learning_rate = float(config['learning_rate'])
    max_epochs = config['max_epochs']
    scheduler_step = config['scheduler_step']
    scheduler_gamma = config['scheduler_gamma']
    early_stop_patience = config['early_stop_patience']

    # set device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # make output directory with the current time-stamp to keep the model
    output_dir = make_directory(output_dir)

    # create a new unet dataset
    ph2_dataset = PH2Dataset(
        root_dir=datapath,
        transform=nn.Sequential(Resize(size_images, size_masks)))

    # Create the corresponding dataloader for training and validation
    data_loader = BaseDataLoader(dataset=ph2_dataset,
                                batch_size=batch_size,
                                validation_split=validation_split,
                                test_split=test_split,
                                random_seed_split=42
                                )

    # Define the model
    unet_model = UNet()

    # Define the loss function and the optimizer
    bce_loss = nn.BCEWithLogitsLoss()
    unet_optimizer = optim.AdamW(unet_model.parameters(), lr=learning_rate)

    # scheduler
    scheduler = optim.lr_scheduler.StepLR(optimizer=unet_optimizer, step_size=scheduler_step, gamma=scheduler_gamma)

    # Initialize the trainer
    basetrainer = BaseTrainer(model = unet_model,
                            loss_function = bce_loss,
                            metric = iou_tgs_challenge,
                            optimizer = unet_optimizer,
                            data_loader = data_loader,
                            n_epochs = max_epochs,
                            lr_sched=scheduler,
                            device = device,
                            early_stop_save_dir=output_dir,
                            early_stop_patience=early_stop_patience,
                            save_dir=output_dir)

    # call tensorboard
    def start_tensorboard(logdir):
        subprocess.call(['tensorboard', '--logdir', logdir])

    tb_thread = threading.Thread(target=start_tensorboard, args=(os.path.join(output_dir, 'summaries'),))
    tb_thread.start()

    # Start training
    train_loss_values, val_loss_values, avg_score_values = basetrainer.train()

```
</details>



